{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "What is Backward Elimination?\n",
    "\n",
    "Backward elimination is a feature selection technique while building a machine learning model. It is used to remove those features that do not have a significant effect on the dependent variable or prediction of output. There are various ways to build a model in Machine Learning, which are:\n",
    "\n",
    "1. All-in\n",
    "2. Backward Elimination\n",
    "3. Forward Selection\n",
    "4. Bidirectional Elimination\n",
    "5. Score Comparison\n",
    "\n",
    "Above are the possible methods for building the model in Machine learning, but we will only use here the Backward Elimination process as it is the fastest method."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Step-1: Firstly, We need to select a significance level to stay in the model. (SL=0.05)\n",
    "\n",
    "Step-2: Fit the complete model with all possible predictors/independent variables.\n",
    "\n",
    "Step-3: Choose the predictor which has the highest P-value, such that.\n",
    "\n",
    "        If P-value >SL, go to step 4.\n",
    "        Else Finish, and Our model is ready.\n",
    "Step-4: Remove that predictor.\n",
    "\n",
    "Step-5: Rebuild and fit the model with the remaining variables.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import statsmodels.api as smf  \n",
    "\n",
    "# this below line is used to add one column containing 1 at the very start. \n",
    "# this is needed because backward elimination consider first column as constant.\n",
    "x = nm.append(arr = nm.ones((50,1)).astype(int), values=x, axis=1)  \n",
    "\n",
    "# x is csv data\n",
    "x_opt=x [:, [0,1,2,3,4,5]]  \n",
    "regressor_OLS=smf.OLS(endog = y, exog=x_opt).fit()  \n",
    "regressor_OLS.summary()  \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In the backward-elimination-in-machine-learning2.png image, we can clearly see the p-values of all the variables. Here x1, x2 are dummy variables, x3 is R&D spend, x4 is Administration spend, and x5 is Marketing spend.\n",
    "\n",
    "From the table, we will choose the highest p-value, which is for x1=0.953 Now, we have the highest p-value which is greater than the SL value(0.5), so we will remove the x1 variable (dummy variable) from the table and will refit the model. Below is the code for it:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "x_opt=x[:, [0,2,3,4,5]]  \n",
    "regressor_OLS=smf.OLS(endog = y, exog=x_opt).fit()  \n",
    "regressor_OLS.summary()  \n",
    "\n",
    "# repeat this untill all p values are less then singnificant level. "
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.8.2 64-bit",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "name": "python",
   "version": "3.8.2"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "31f2aee4e71d21fbe5cf8b01ff0e069b9275f58929596ceb00d14d90e3e16cd6"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
