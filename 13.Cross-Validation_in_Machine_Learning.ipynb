{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h2>Cross-Validation in Machine Learning</h2>\n",
    "\n",
    "In machine learning, there is always the need to test the stability of the model. It means based only on the training dataset; we can't fit our model on the training dataset. For this purpose, we reserve a particular sample of the dataset, which was not part of the training dataset. After that, we test our model on that sample before deployment, and this complete process comes under cross-validation.\n",
    "\n",
    "<h3>Methods used for Cross-Validation</h3>\n",
    "\n",
    "- Validation Set Approach</br>\n",
    "- Leave-P-out cross-validation</br>\n",
    "- Leave one out cross-validation</br>\n",
    "- K-fold cross-validation</br>\n",
    "- Stratified k-fold cross-validation</br>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h3>Validation Set Approach</h3>\n",
    "\n",
    "We divide our input dataset into a training set and test or validation set in the validation set approach. Both the subsets are given 50% of the dataset.\n",
    "\n",
    "<h3>Leave-P-out cross-validation</h3>\n",
    "\n",
    "In this approach, the p datasets are left out of the training data. It means, if there are total n datapoints in the original input dataset, then n-p data points will be used as the training dataset and the p data points as the validation set. This complete process is repeated for all the samples, and the average error is calculated to know the effectiveness of the model.\n",
    "\n",
    "\n",
    "<h3>Leave one out cross-validation</h3>\n",
    "\n",
    "This method is similar to the leave-p-out cross-validation, but instead of p, we need to take 1 dataset out of training. It means, in this approach, for each learning set, only one datapoint is reserved, and the remaining dataset is used to train the model. This process repeats for each datapoint. Hence for n samples, we get n different training set and n test set.\n",
    "\n",
    "<h3>K-Fold Cross-Validation</h3>\n",
    "\n",
    "K-fold cross-validation approach divides the input dataset into K groups of samples of equal sizes. These samples are called folds. For each learning set, the prediction function uses k-1 folds, and the rest of the folds are used for the test set. This approach is a very popular CV approach because it is easy to understand, and the output is less biased than other methods.\n",
    "\n",
    "\n",
    "<h3>Stratified k-fold cross-validation</h3>\n",
    "\n",
    "This technique is similar to k-fold cross-validation with some little changes. This approach works on stratification concept, it is a process of rearranging the data to ensure that each fold or group is a good representative of the complete dataset. To deal with the bias and variance, it is one of the best approaches."
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
